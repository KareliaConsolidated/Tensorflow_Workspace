{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Input and Target Variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = diabetes_dataset['data']\n",
    "targets = diabetes_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (397, 10)\n",
      "Test Data Shape: (45, 10)\n",
      "Train Target Shape: (397,)\n",
      "Test Target Shape: (45,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data Shape: {train_data.shape}\")\n",
    "print(f\"Test Data Shape: {test_data.shape}\")\n",
    "print(f\"Train Target Shape: {train_targets.shape}\")\n",
    "print(f\"Test Target Shape: {test_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we construct our Model.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we compile our model\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Callback:\n",
    "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    # Print the loss after every second batch in the training set\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 2 == 0:\n",
    "            print(f\"\\n After batch {batch}, the loss is {logs['loss']:7.2f}\")\n",
    "    \n",
    "    # Print the loss after each batch in the test set\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
    "\n",
    "    # Print the loss and mean absolute error after each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n",
    "    \n",
    "    # Notify the user when prediction has finished on each batch\n",
    "    def on_predict_batch_end(self,batch, logs=None):\n",
    "        print(\"Finished prediction on batch {}!\".format(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 29893.04\n",
      "\n",
      " After batch 2, the loss is 28118.86\n",
      "Epoch 0: Average loss is 28629.21, mean absolute error is  151.34.\n",
      "\n",
      " After batch 0, the loss is 27317.28\n",
      "\n",
      " After batch 2, the loss is 27987.85\n",
      "Epoch 1: Average loss is 28496.76, mean absolute error is  150.95.\n",
      "\n",
      " After batch 0, the loss is 24041.78\n",
      "\n",
      " After batch 2, the loss is 28358.69\n",
      "Epoch 2: Average loss is 28309.87, mean absolute error is  150.39.\n",
      "\n",
      " After batch 0, the loss is 33287.23\n",
      "\n",
      " After batch 2, the loss is 29390.03\n",
      "Epoch 3: Average loss is 28041.65, mean absolute error is  149.56.\n",
      "\n",
      " After batch 0, the loss is 25017.11\n",
      "\n",
      " After batch 2, the loss is 27015.26\n",
      "Epoch 4: Average loss is 27634.07, mean absolute error is  148.33.\n",
      "\n",
      " After batch 0, the loss is 29015.75\n",
      "\n",
      " After batch 2, the loss is 27236.49\n",
      "Epoch 5: Average loss is 27096.89, mean absolute error is  146.67.\n",
      "\n",
      " After batch 0, the loss is 27933.21\n",
      "\n",
      " After batch 2, the loss is 27459.71\n",
      "Epoch 6: Average loss is 26363.26, mean absolute error is  144.39.\n",
      "\n",
      " After batch 0, the loss is 23232.44\n",
      "\n",
      " After batch 2, the loss is 25706.67\n",
      "Epoch 7: Average loss is 25401.35, mean absolute error is  141.35.\n",
      "\n",
      " After batch 0, the loss is 25559.31\n",
      "\n",
      " After batch 2, the loss is 24064.98\n",
      "Epoch 8: Average loss is 24201.48, mean absolute error is  137.46.\n",
      "\n",
      " After batch 0, the loss is 24709.54\n",
      "\n",
      " After batch 2, the loss is 22201.27\n",
      "Epoch 9: Average loss is 22713.88, mean absolute error is  132.41.\n",
      "\n",
      " After batch 0, the loss is 22223.82\n",
      "\n",
      " After batch 2, the loss is 21042.83\n",
      "Epoch 10: Average loss is 20913.25, mean absolute error is  126.07.\n",
      "\n",
      " After batch 0, the loss is 16572.78\n",
      "\n",
      " After batch 2, the loss is 18115.78\n",
      "Epoch 11: Average loss is 18817.58, mean absolute error is  118.30.\n",
      "\n",
      " After batch 0, the loss is 16499.31\n",
      "\n",
      " After batch 2, the loss is 16634.21\n",
      "Epoch 12: Average loss is 16531.75, mean absolute error is  109.06.\n",
      "\n",
      " After batch 0, the loss is 12098.93\n",
      "\n",
      " After batch 2, the loss is 14169.29\n",
      "Epoch 13: Average loss is 14167.16, mean absolute error is   98.87.\n",
      "\n",
      " After batch 0, the loss is 11477.12\n",
      "\n",
      " After batch 2, the loss is 11737.60\n",
      "Epoch 14: Average loss is 11736.07, mean absolute error is   87.52.\n",
      "\n",
      " After batch 0, the loss is 11229.04\n",
      "\n",
      " After batch 2, the loss is 9635.77\n",
      "Epoch 15: Average loss is 9639.28, mean absolute error is   77.50.\n",
      "\n",
      " After batch 0, the loss is 9524.17\n",
      "\n",
      " After batch 2, the loss is 8228.67\n",
      "Epoch 16: Average loss is 7958.45, mean absolute error is   69.20.\n",
      "\n",
      " After batch 0, the loss is 8543.99\n",
      "\n",
      " After batch 2, the loss is 7007.47\n",
      "Epoch 17: Average loss is 6745.85, mean absolute error is   63.36.\n",
      "\n",
      " After batch 0, the loss is 6480.73\n",
      "\n",
      " After batch 2, the loss is 6234.43\n",
      "Epoch 18: Average loss is 5987.93, mean absolute error is   59.29.\n",
      "\n",
      " After batch 0, the loss is 4255.94\n",
      "\n",
      " After batch 2, the loss is 5402.50\n",
      "Epoch 19: Average loss is 5845.36, mean absolute error is   60.42.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "history = model.fit(train_data, train_targets, epochs=20, batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " After batch 0, the loss is 16615.33.\n",
      "\n",
      " After batch 1, the loss is 21633.04.\n",
      "\n",
      " After batch 2, the loss is 19110.87.\n",
      "\n",
      " After batch 3, the loss is 19054.55.\n",
      "\n",
      " After batch 4, the loss is 19003.15.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "model_eval = model.evaluate(test_data, test_targets, batch_size=10, callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction on batch 0!\n",
      "Finished prediction on batch 1!\n",
      "Finished prediction on batch 2!\n",
      "Finished prediction on batch 3!\n",
      "Finished prediction on batch 4!\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the model\n",
    "\n",
    "model_pred = model.predict(test_data, batch_size=10,\n",
    "                           callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application - learning rate scheduler\n",
    "Let's now look at a more sophisticated custom callback. \n",
    "\n",
    "We are going to define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n",
    "\n",
    "First we define the auxillary function that returns the learning rate for each epoch based on our schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate schedule. The tuples below are (start_epoch, new_learning_rate)\n",
    "\n",
    "lr_schedule = [(4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)]\n",
    "\n",
    "def get_new_epoch_lr(epoch, lr):\n",
    "    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0] == int(epoch)]\n",
    "    if len(epoch_in_sched) > 0:\n",
    "        return lr_schedule[epoch_in_sched[0]][1]\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom callback\n",
    "\n",
    "class LRScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, new_lr):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        # Add the new learning rate function to our callback\n",
    "        self.new_lr = new_lr\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "              raise ValueError('Error: Optimizer does not have a learning rate.')\n",
    "                \n",
    "        # Get the current learning rate\n",
    "        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        # Call the auxillary function to get the scheduled learning rate for the current epoch\n",
    "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
    "\n",
    "        # Set the learning rate to the scheduled learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
    "        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the same model as before\n",
    "\n",
    "new_model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n",
    "    Dense(64,activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "new_model.compile(loss='mse',\n",
    "                optimizer=\"adam\",\n",
    "                metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate for epoch 0 is   0.001\n",
      "Learning rate for epoch 1 is   0.001\n",
      "Learning rate for epoch 2 is   0.001\n",
      "Learning rate for epoch 3 is   0.001\n",
      "Learning rate for epoch 4 is   0.030\n",
      "Learning rate for epoch 5 is   0.030\n",
      "Learning rate for epoch 6 is   0.030\n",
      "Learning rate for epoch 7 is   0.020\n",
      "Learning rate for epoch 8 is   0.020\n",
      "Learning rate for epoch 9 is   0.020\n",
      "Learning rate for epoch 10 is   0.020\n",
      "Learning rate for epoch 11 is   0.005\n",
      "Learning rate for epoch 12 is   0.005\n",
      "Learning rate for epoch 13 is   0.005\n",
      "Learning rate for epoch 14 is   0.005\n",
      "Learning rate for epoch 15 is   0.007\n",
      "Learning rate for epoch 16 is   0.007\n",
      "Learning rate for epoch 17 is   0.007\n",
      "Learning rate for epoch 18 is   0.007\n",
      "Learning rate for epoch 19 is   0.007\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with our learning rate scheduler callback\n",
    "\n",
    "new_history = new_model.fit(train_data, train_targets, epochs=20,\n",
    "                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
